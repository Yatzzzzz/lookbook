'use server'; import { VertexAI } from '@google-cloud/aiplatform'; import { NextResponse } from 'next/server'; // Initialize Vertex AI (you will need to configure your project and region details) const clientOptions = { project: 'lookbook-454113', // Replace with your Google Cloud Project ID location: 'us-central1', // Replace with your region (e.g., us-central1) }; const vertexAI = new VertexAI(clientOptions); // Name of the Gemini model to use const modelName = 'gemini-pro-vision'; export async function POST(req) { try { const { image, question } = await req.json(); const response = await processImageAndQuestion(image, question); return NextResponse.json({ result: response }); } catch (error) { console.error('Error processing the request:', error); return NextResponse.json({ error: 'An error occurred while processing the request' }, { status: 500 }); } } async function processImageAndQuestion(image, question) { if (!image) { return 'No image data received.'; } const model = vertexAI.preview.generativeModel({ model: modelName, }); const generationConfig = { maxOutputTokens: 2048, // Adjust as needed temperature: 0.4, // Adjust for more or less creative responses topP: 1, }; const parts = [ { inlineData: { data: image.split(',')[1], // Remove the data URL prefix mimeType: 'image/png', // Adjust mimeType if necessary }, }, { text: question }, ]; try { const result = await model.generateContent({ contents: [{ role: 'user', parts }], generationConfig, }); const response = result.response?.candidates?.[0]?.content?.parts?.[0]?.text; return response || 'No response from the model.'; } catch (error) { console.error('Error calling Gemini Pro:', error); return 'Error communicating with the AI model.'; } }